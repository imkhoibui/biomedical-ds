[
  {
    "objectID": "inference/power-cal.html",
    "href": "inference/power-cal.html",
    "title": "Power calculations",
    "section": "",
    "text": "Type I error refers to the instance where we reject the null when p-value is less than 0.05 (1 out of 20). P-value is mostly never 0, we are just saying that the chance where we got to reject it when it’s true is relatively small. This is also called false positive.\nOn the other hand, I pondered about, “well if 0.05 implies that there’s a chance we might be rejecting the null hypothesis even when it’s true, why don’t we just set p-value to be incredibly small, like 0.0001 for instance?”. In this case, you and I might be missing out on NOT rejecting the null hypothesis when we should. This is called type II error.",
    "crumbs": [
      "About",
      "Inference",
      "Power calculations"
    ]
  },
  {
    "objectID": "inference/power-cal.html#power-calculations",
    "href": "inference/power-cal.html#power-calculations",
    "title": "Power calculations",
    "section": "Power calculations",
    "text": "Power calculations\n\nreject &lt;- function(N, alpha=0.05) {\n    hf &lt;- sample(hfPopulation, N)\n    control &lt;- sample(controlPopulation, N)\n    pval &lt;- t.test(hf, control)$p.value\n    pval &lt; alpha\n}\n\n\nB &lt;- 2000\nN &lt;- 12\nrejections &lt;- replicate(B, reject(N))\nmean(rejections)\n\n[1] 0.2245\n\n\nWe can see that with a replication of drawing samples from hf and control populations, each with only 12 individuals per sample, the percentile in which we reject the null hypothesis out of 2000 repetitions is pretty low.\nLet’s see how the change in power improves with larger sample size.\n\nNs &lt;- seq(5, 50, 5)\n\npower &lt;- sapply(Ns, function(N) {\n    rejections &lt;- replicate(B, reject(N))\n    mean(rejections)\n})\nplot(Ns, power, type = \"b\")\n\n\n\n\n\n\n\n\nWe can see that with a sample size of 12, the result gave us the mean similar to what we calculated before, and as the sample size grows, the power is increasing accordingly.\nThe same power change can be observed if we instead change the alpha:\n\nN &lt;- 30\nalphas &lt;- c(0.1,0.05,0.01,0.001,0.0001)\npower &lt;- sapply(alphas, function(alpha){\n  rejections &lt;- replicate(B, reject(N, alpha=alpha))\n  mean(rejections)\n})\nplot(alphas, power, xlab=\"alpha\", type=\"b\", log=\"x\")",
    "crumbs": [
      "About",
      "Inference",
      "Power calculations"
    ]
  },
  {
    "objectID": "inference/introduction.html",
    "href": "inference/introduction.html",
    "title": "Introduction on Inference",
    "section": "",
    "text": "In this section, I learned about basic data reading in an experiment:  - Dplyr library is a powerful package used for manipulating data frames in R.  - We start by loading this package.\n\nlibrary(downloader)\nlibrary(dplyr)\n\nThe example for the rest of this chapter was retrived from the genomic class’s data.  Here’s how I download the dataset\n\nurl &lt;- \"https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv\"\nfilename &lt;- \"../data/femaleMiceWeights.csv\"\ndownload(url, destfile=filename)\n\ndata &lt;- read.csv(filename)\n\nWe can take a look at the data’s first few rows\n\nhead(data)\n\n  Diet Bodyweight\n1 chow      21.51\n2 chow      28.14\n3 chow      24.04\n4 chow      23.45\n5 chow      23.68\n6 chow      19.79\n\n\nFor this example, I drew all the control female mice (chow) and treatment female mice (hf) into different data frames from the original dataset. Referring to the paper, the authors hypothesized that mice with treatments - a different, more fat diet would have more weights averagely than those without the treatment.  This paper was published in 2004, so the hypothesis and outcome was groundbreaking!?\n\ncontrol &lt;- filter(data, Diet == \"chow\") %&gt;% select(Bodyweight) %&gt;% unlist\ntreatment &lt;- filter(data, Diet == \"hf\") %&gt;% select(Bodyweight) %&gt;% unlist\n\n\nprint(mean(treatment))\n\n[1] 26.83417\n\nprint(mean(control))\n\n[1] 23.81333\n\nobsdiff &lt;- mean(treatment) - mean(control)\nprint(obsdiff)\n\n[1] 3.020833\n\n\nAs we could see, the differences in the treatment was noticable when we drew from a big enough population. As I have learned, this is an indication that we can use the data earned to prove/disprove the hypothesis: “Female mice that were fed a high fat diet would earn more weights than those who weren’t”.\nHowever, the data sampled was very random (hence the name random variables), what if I sample a different population and the results is the opposite (high fat diet makes female mice lose more weight?). The only way to make sure of this is to try and sample from a greater population, or more times, so that the results obtained are consolidated with each attempt.\nI guess that is the basic of sampling and random variable to me.",
    "crumbs": [
      "About",
      "Inference",
      "Introduction on Inference"
    ]
  },
  {
    "objectID": "inference/introduction.html#introduction",
    "href": "inference/introduction.html#introduction",
    "title": "Introduction on Inference",
    "section": "",
    "text": "In this section, I learned about basic data reading in an experiment:  - Dplyr library is a powerful package used for manipulating data frames in R.  - We start by loading this package.\n\nlibrary(downloader)\nlibrary(dplyr)\n\nThe example for the rest of this chapter was retrived from the genomic class’s data.  Here’s how I download the dataset\n\nurl &lt;- \"https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv\"\nfilename &lt;- \"../data/femaleMiceWeights.csv\"\ndownload(url, destfile=filename)\n\ndata &lt;- read.csv(filename)\n\nWe can take a look at the data’s first few rows\n\nhead(data)\n\n  Diet Bodyweight\n1 chow      21.51\n2 chow      28.14\n3 chow      24.04\n4 chow      23.45\n5 chow      23.68\n6 chow      19.79\n\n\nFor this example, I drew all the control female mice (chow) and treatment female mice (hf) into different data frames from the original dataset. Referring to the paper, the authors hypothesized that mice with treatments - a different, more fat diet would have more weights averagely than those without the treatment.  This paper was published in 2004, so the hypothesis and outcome was groundbreaking!?\n\ncontrol &lt;- filter(data, Diet == \"chow\") %&gt;% select(Bodyweight) %&gt;% unlist\ntreatment &lt;- filter(data, Diet == \"hf\") %&gt;% select(Bodyweight) %&gt;% unlist\n\n\nprint(mean(treatment))\n\n[1] 26.83417\n\nprint(mean(control))\n\n[1] 23.81333\n\nobsdiff &lt;- mean(treatment) - mean(control)\nprint(obsdiff)\n\n[1] 3.020833\n\n\nAs we could see, the differences in the treatment was noticable when we drew from a big enough population. As I have learned, this is an indication that we can use the data earned to prove/disprove the hypothesis: “Female mice that were fed a high fat diet would earn more weights than those who weren’t”.\nHowever, the data sampled was very random (hence the name random variables), what if I sample a different population and the results is the opposite (high fat diet makes female mice lose more weight?). The only way to make sure of this is to try and sample from a greater population, or more times, so that the results obtained are consolidated with each attempt.\nI guess that is the basic of sampling and random variable to me.",
    "crumbs": [
      "About",
      "Inference",
      "Introduction on Inference"
    ]
  },
  {
    "objectID": "inference/introduction.html#random-variable",
    "href": "inference/introduction.html#random-variable",
    "title": "Introduction on Inference",
    "section": "Random variable",
    "text": "Random variable\nNow let’s draw from a regular, no diet population.\n\nurl &lt;- \"https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv\"\nfilename &lt;- \"../data/femaleControlsPopulation.csv\"\ndownload(url, destfile=filename)\n\npopulation &lt;- read.csv(filename)\npopulation &lt;- unlist(population)\n\nNow we can try to sample the data many time\n\ncontrol &lt;- sample(population, 12)\nprint(paste(\"Drawing once\", mean(control)))\n\n[1] \"Drawing once 25.3241666666667\"\n\ncontrol &lt;- sample(population, 12)\nprint(paste(\"Drawing twice\", mean(control)))\n\n[1] \"Drawing twice 24.8333333333333\"\n\ncontrol &lt;- sample(population, 12)\nprint(paste(\"Drawing for the third time\", mean(control)))\n\n[1] \"Drawing for the third time 22.8433333333333\"\n\n\nEach sample gives a different mean of the control population. Hmmm, surely there must be a way for us to make sense of the sample average in relation to the population average?",
    "crumbs": [
      "About",
      "Inference",
      "Introduction on Inference"
    ]
  },
  {
    "objectID": "inference/introduction.html#the-null-hypothesis",
    "href": "inference/introduction.html#the-null-hypothesis",
    "title": "Introduction on Inference",
    "section": "The Null Hypothesis",
    "text": "The Null Hypothesis\nEverytime we establish a hypothesis, we must concurrently establish a null hypothesis, which is the negated version of what we hypothesized. This is crucial in science, since if you want to test for something, how do you prove that it’s true? We don’t actually, the only way to test is to try and disprove it, not to prove it, as we want to make progress and advancement through “fixing” our old understanding of the subject. Science is about falsifiability, if something is omni-correct, then it’s not science, it’s not quantifiable, it’s not possible to be debunked.\n\nsampling_times &lt;- 10000\nnull &lt;- vector(\"numeric\", sampling_times)\n\nfor (i in 1:sampling_times) {\n    control &lt;- sample(population, 12)\n    treatment &lt;- sample(population, 12)\n    null[i] &lt;- mean(treatment) - mean(control)\n}\n\nprint(mean(null &gt;= obsdiff))\n\n[1] 0.0143\n\n\nAs we can see, there’s not a lot of percentage, out of the 10000 samples, a very small percentile shows greater difference than the difference between control and treatment group above. Informally, this is known as p-value.",
    "crumbs": [
      "About",
      "Inference",
      "Introduction on Inference"
    ]
  },
  {
    "objectID": "inference/introduction.html#probability-distributions",
    "href": "inference/introduction.html#probability-distributions",
    "title": "Introduction on Inference",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nDistributions as my understanding, is a way to measure and make sense of the differences/variances among data within a specific metric/group. Say you have data of patients’ ages who visited the Cho Ray hospital in 2023, what can you say/describe in an overall fashion about the data? One such way is to use a CDF (cumulative distribution function).\nLet’s start with a height example (because sadly I don’t have the data from Cho Ray hospital).\n\nround(sample(x, 10), 1)\n\n [1] 64.7 64.6 69.3 69.3 68.6 65.1 69.2 68.6 69.6 69.6\n\nsmallest &lt;- floor(min(x))\nlargest &lt;- ceiling(max(x))\nvalues &lt;- seq(smallest, largest, len=300)\nheightecdf &lt;- ecdf(x)\nplot(values, heightecdf(values), type=\"l\",\n    xlab=\"a (Height in inches)\", ylab=\"Pr(x &lt;= a)\")\n\n\n\n\n\n\n\n\nThe probability function explains that, “we start out with 0 example and 0 CDF, as the number of examples increase, the number of CDF also increase but you can kinda tell where most of the data lies within those examples”.\nSo for this data, no one was observed under ~58 inches, but there are a few at the height increases, and no further observation is greater than ~77 inches.\nHistogram is another great way to illustrate the data:\n\nbins &lt;- seq(smallest, largest)\nhist(x, breaks = bins, xlab = \"Height (in inches)\", main = \"Adult heights\")",
    "crumbs": [
      "About",
      "Inference",
      "Introduction on Inference"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name is Khoi, I am currently a senior student (about to graduate!) at Fulbright University Vietnam.\nI major in Computer Science, with a minor in Integrated Science. During my time here, I have discovered my joy of working with biomedical data. I am self-learning a lot in my free time and what you are seeing here is a prime example of one.\nThis website is a notebook for my learning of the course “Biomedical Data Science” provided by MIT OpenCoursework.\nThe original access to the webpage can be found here.\nI do not own any of the information provided by them, yet I tried to adapt some of the materials into my own learning. If there’s any violation of the materials or I am using any information not owned by me, you are welcome to reach out to me.\nThank you for reading, I hope you’d found something useful!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sections covered:\n\n1. Inference   This section covers the foundational terms related to biomedical data science.  I ponder upon the learned concepts from my MATH205 - Probability course, and I attempt to revisit them in this section.\nWhat is a probability distribution? What makes a hypothesis rejected? What is the intuition behind statistical tests?…\nThose are the questions that should be found within this section.\n 2. Exploratory Data Analysis \n 3. Robust Statistics"
  },
  {
    "objectID": "inference/clt.html",
    "href": "inference/clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "CLT is one of the most prominent theorem used in scientific computing. It tells us that once the sample size is large enough, the average of a random sample follows a normal distribution centered at the population average.\nWe can refer to the standard deviation of the distribution of a random variable as the random variable’s standard error.\nAn example for this could be the uniform change in each sample shifts the entire sample’s mean and sd by the same amount. If each mice has its own weight deducted by 10 gram, then the entire sample’s average weight would be reduced by 10 gram. Intuitively, the equation for this would be:\n\\[\\frac{\\bar{Y} - \\mu}{\\sigma_{Y}/\\sqrt{N}}\\]",
    "crumbs": [
      "About",
      "Inference",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "inference/clt.html#central-limit-theorem",
    "href": "inference/clt.html#central-limit-theorem",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "CLT is one of the most prominent theorem used in scientific computing. It tells us that once the sample size is large enough, the average of a random sample follows a normal distribution centered at the population average.\nWe can refer to the standard deviation of the distribution of a random variable as the random variable’s standard error.\nAn example for this could be the uniform change in each sample shifts the entire sample’s mean and sd by the same amount. If each mice has its own weight deducted by 10 gram, then the entire sample’s average weight would be reduced by 10 gram. Intuitively, the equation for this would be:\n\\[\\frac{\\bar{Y} - \\mu}{\\sigma_{Y}/\\sqrt{N}}\\]",
    "crumbs": [
      "About",
      "Inference",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "inference/clt.html#t-distribution",
    "href": "inference/clt.html#t-distribution",
    "title": "Central Limit Theorem",
    "section": "t-distribution",
    "text": "t-distribution\nCLT works in mostly large samples, in cases where CLT does not apply, how do we measure whether a hypothesis could be rejected or not?\nT-distribution (or Student’s t-distribution) is considered when the sample size does not meet large requirements.\nLet’s load an example:\n\nlibrary(dplyr)\nlibrary(rafalib)\nlibrary(downloader)\nurl &lt;- \"https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv\"\nfilename &lt;- \"../data/mice_pheno.csv\"\ndownload(url, destfile=filename)\n\n\ndat &lt;- read.csv(\"../data/mice_pheno.csv\")\ncontrolPopulation &lt;- filter(dat, Sex == \"F\" & Diet == \"chow\") %&gt;%  \n  select(Bodyweight) %&gt;% unlist\nhfPopulation &lt;- filter(dat,Sex == \"F\" & Diet == \"hf\") %&gt;%  \n  select(Bodyweight) %&gt;% unlist\n\n\nmypar(1,2)\nhist(hfPopulation)\nhist(controlPopulation)\n\n\n\n\n\n\n\n\nLet’s perform quartile-quartile plot to compare the distributions being closer to normal distribution.\n\nmypar(1,2)\nqqnorm(hfPopulation)\nqqline(hfPopulation)\nqqnorm(controlPopulation)\nqqline(controlPopulation)\n\n\n\n\n\n\n\n\n\nmu_hf &lt;- mean(hfPopulation)\nmu_control &lt;- mean(controlPopulation)\nprint(mu_hf - mu_control)\n\n[1] 2.375517\n\nsd_hf &lt;- popsd(hfPopulation)\nsd_control &lt;- popsd(controlPopulation)\n\n\nN &lt;- 12\nhf &lt;- sample(hfPopulation, 12)\ncontrol &lt;- sample(controlPopulation, 12)\n\n\nNs &lt;- c(3,12,25,50)\nB &lt;- 10000\nres &lt;-  sapply(Ns,function(n) {\n  replicate(B, mean(sample(hfPopulation, n)) - mean(sample(controlPopulation, n)))\n})\n\n\nmypar(2,2)\nfor (i in seq(along=Ns)) {\n  titleavg &lt;- signif(mean(res[,i]),3)\n  titlesd &lt;- signif(popsd(res[,i]),3)\n  title &lt;- paste0(\"N=\",Ns[i],\" Avg=\",titleavg,\" SD=\",titlesd)\n  qqnorm(res[,i],main=title)\n  qqline(res[,i],col=2)\n}",
    "crumbs": [
      "About",
      "Inference",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "inference/monte-carlo.html",
    "href": "inference/monte-carlo.html",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "library(dplyr)\ndata &lt;- read.csv(\"../data/mice_pheno.csv\")\ncontrolPopulation &lt;- filter(data, Sex == \"F\" & Diet == \"chow\") %&gt;% \n    select(Bodyweight) %&gt;% unlist\n\n\nttestgenerator &lt;- function(n) {\n    cases &lt;- sample(controlPopulation, n)\n    controls &lt;- sample(controlPopulation, n)\n    tstat &lt;- (mean(cases) - mean(controls)) / sqrt(var(cases)/n + var(controls)/n)\n    return(tstat)\n}\n\nttests &lt;- replicate(1000, ttestgenerator(10))\nhist(ttests)\n\n\n\n\n\n\n\n\n\nqqnorm(ttests)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nps &lt;- (seq(0, 999) + 0.5)/1000\nqqplot(qt(ps, df = 2*3-2), ttests, xlim=c(-6, 6), ylim=c(-6, 6))\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nqqnorm(controlPopulation)\nqqline(controlPopulation)",
    "crumbs": [
      "About",
      "Inference",
      "Monte Carlo methods"
    ]
  },
  {
    "objectID": "inference/monte-carlo.html#monte-carlo-simulation",
    "href": "inference/monte-carlo.html#monte-carlo-simulation",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "library(dplyr)\ndata &lt;- read.csv(\"../data/mice_pheno.csv\")\ncontrolPopulation &lt;- filter(data, Sex == \"F\" & Diet == \"chow\") %&gt;% \n    select(Bodyweight) %&gt;% unlist\n\n\nttestgenerator &lt;- function(n) {\n    cases &lt;- sample(controlPopulation, n)\n    controls &lt;- sample(controlPopulation, n)\n    tstat &lt;- (mean(cases) - mean(controls)) / sqrt(var(cases)/n + var(controls)/n)\n    return(tstat)\n}\n\nttests &lt;- replicate(1000, ttestgenerator(10))\nhist(ttests)\n\n\n\n\n\n\n\n\n\nqqnorm(ttests)\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nps &lt;- (seq(0, 999) + 0.5)/1000\nqqplot(qt(ps, df = 2*3-2), ttests, xlim=c(-6, 6), ylim=c(-6, 6))\nabline(0, 1)\n\n\n\n\n\n\n\n\n\nqqnorm(controlPopulation)\nqqline(controlPopulation)",
    "crumbs": [
      "About",
      "Inference",
      "Monte Carlo methods"
    ]
  },
  {
    "objectID": "inference/monte-carlo.html#parametric-simulations-for-the-observations",
    "href": "inference/monte-carlo.html#parametric-simulations-for-the-observations",
    "title": "Monte Carlo methods",
    "section": "Parametric Simulations for the Observations",
    "text": "Parametric Simulations for the Observations\n\ncontrols &lt;- rnorm(5000, mean=24, sd=3.5)\n\nttestgenerator &lt;- function(n, mean=24, sd=3.5) {\n    cases &lt;- rnorm(n, mean, sd)\n    controls &lt;- rnorm(n, mean, sd)\n    tstat &lt;- (mean(cases)-mean(controls)) / \n      sqrt(var(cases)/n + var(controls)/n) \n    return(tstat)\n}",
    "crumbs": [
      "About",
      "Inference",
      "Monte Carlo methods"
    ]
  }
]